import numpy as np
import torch
import time
import math
import json
from scipy.special import softmax

from umap.umap_ import fuzzy_simplicial_set
from pynndescent import NNDescent
from sklearn.neighbors import NearestNeighbors
from sklearn.utils import check_random_state

from singleVis.kcenter_greedy import kCenterGreedy
from singleVis.intrinsic_dim import IntrinsicDim
from singleVis.backend import get_graph_elements, get_attention


'''Base class for Spatial Edge Constructor'''
class SpatialEdgeConstructor:
    '''Construct spatial complex
    '''
    def __init__(self, data_provider, init_num, n_epochs, n_neighbors) -> None:
        """Init parameters for spatial edge constructor

        Parameters
        ----------
        data_provider : data.DataProvider
             data provider
        init_num : int
            init number to calculate c
        n_epochs : int
            the number of epochs to fit for one iteration(epoch)
            e.g. n_epochs=5 means each edge will be sampled 5*prob times in one training epoch
        n_neighbors: int
            local connectivity
        """
        self.data_provider = data_provider
        self.init_num = init_num
        self.n_epochs = n_epochs
        self.n_neighbors = n_neighbors
    
    def _construct_fuzzy_complex(self, train_data):
        """
        construct a vietoris-rips complex
        """
        # number of trees in random projection forest
        n_trees = min(64, 5 + int(round((train_data.shape[0]) ** 0.5 / 20.0)))
        # max number of nearest neighbor iters to perform
        n_iters = max(5, int(round(np.log2(train_data.shape[0]))))
        # distance metric
        metric = "euclidean"
        # get nearest neighbors
        nnd = NNDescent(
            train_data,
            n_neighbors=self.n_neighbors,
            metric=metric,
            n_trees=n_trees,
            n_iters=n_iters,
            max_candidates=60,
            verbose=True
        )
        knn_indices, knn_dists = nnd.neighbor_graph
        random_state = check_random_state(None)
        complex, sigmas, rhos = fuzzy_simplicial_set(
            X=train_data,
            n_neighbors=self.n_neighbors,
            metric=metric,
            random_state=random_state,
            knn_indices=knn_indices,
            knn_dists=knn_dists,
        )
        return complex, sigmas, rhos, knn_indices
    
    def _construct_boundary_wise_complex(self, train_data, border_centers):
        """compute the boundary wise complex
            for each border point, we calculate its k nearest train points
            for each train data, we calculate its k nearest border points
        """
        high_neigh = NearestNeighbors(n_neighbors=self.n_neighbors, radius=0.4)
        high_neigh.fit(border_centers)
        fitting_data = np.concatenate((train_data, border_centers), axis=0)
        knn_dists, knn_indices = high_neigh.kneighbors(fitting_data, n_neighbors=self.n_neighbors, return_distance=True)
        knn_indices = knn_indices + len(train_data)

        random_state = check_random_state(None)
        bw_complex, sigmas, rhos = fuzzy_simplicial_set(
            X=fitting_data,
            n_neighbors=self.n_neighbors,
            metric="euclidean",
            random_state=random_state,
            knn_indices=knn_indices,
            knn_dists=knn_dists,
        )
        return bw_complex, sigmas, rhos, knn_indices
    
    def _construct_step_edge_dataset(self, vr_complex, bw_complex):
        """
        construct the mixed edge dataset for one time step
            connect border points and train data(both direction)
        :param vr_complex: Vietoris-Rips complex
        :param bw_complex: boundary-augmented complex
        :param n_epochs: the number of epoch that we iterate each round
        :return: edge dataset
        """
        # get data from graph
        _, vr_head, vr_tail, vr_weight, _ = get_graph_elements(vr_complex, self.n_epochs)
        # get data from graph
        _, bw_head, bw_tail, bw_weight, _ = get_graph_elements(bw_complex, 1)

        head = np.concatenate((vr_head, bw_head), axis=0)
        tail = np.concatenate((vr_tail, bw_tail), axis=0)
        weight = np.concatenate((vr_weight, bw_weight), axis=0)

        return head, tail, weight
        # return vr_head, vr_tail, vr_weight
    

    def construct(self):
        return NotImplemented

'''
Two strategies:
    Random: random select samples
    KC: select coreset using k center greedy algorithm
'''

class RandomSpatialEdgeConstructor(SpatialEdgeConstructor):
    def __init__(self, data_provider, init_num, n_epochs) -> None:
        super().__init__(data_provider, init_num, n_epochs)
    
    def construct(self):
        # dummy input
        edge_to = None
        edge_from = None
        sigmas = None
        rhos = None
        weight = None
        probs = None
        feature_vectors = None
        attention = None
        knn_indices = None
        time_step_nums = list()
        time_step_idxs_list = list()

        train_num = self.data_provider.train_num
        selected_idxs = np.random.choice(np.arange(train_num), size=self.init_num, replace=False)
        selected_idxs_t = np.array(range(len(selected_idxs)))

        # each time step
        for t in range(self.data_provider.s, self.data_provider.e+1, self.data_provider.p):
            # load train data and border centers
            train_data = self.data_provider.train_representation(t).squeeze()

            train_data = train_data[selected_idxs]
            time_step_idxs_list.append(selected_idxs_t.tolist())

            selected_idxs_t = np.random.choice(list(range(len(selected_idxs))), int(0.9*len(selected_idxs)), replace=False)
            selected_idxs = selected_idxs[selected_idxs_t]

            border_centers = self.data_provider.border_representation(t).squeeze()
            border_centers = border_centers

            complex, sigmas_t1, rhos_t1, knn_idxs_t = self._construct_fuzzy_complex(train_data)
            bw_complex, sigmas_t2, rhos_t2, _ = self._construct_boundary_wise_complex(train_data, border_centers)
            edge_to_t, edge_from_t, weight_t = self._construct_step_edge_dataset(complex, bw_complex, self.n_epochs)
            sigmas_t = np.concatenate((sigmas_t1, sigmas_t2[len(sigmas_t1):]), axis=0)
            rhos_t = np.concatenate((rhos_t1, rhos_t2[len(rhos_t1):]), axis=0)
            fitting_data = np.concatenate((train_data, border_centers), axis=0)
            pred_model = self.data_provider.prediction_function(t)
            attention_t = get_attention(pred_model, fitting_data, temperature=.01, device=self.data_provider.DEVICE, verbose=1)
            t_num = len(train_data)
            b_num = len(border_centers)

            if edge_to is None:
                edge_to = edge_to_t
                edge_from = edge_from_t
                weight = weight_t
                probs = weight_t / weight_t.max()
                feature_vectors = fitting_data
                attention = attention_t
                sigmas = sigmas_t
                rhos = rhos_t
                knn_indices = knn_idxs_t
                time_step_nums.append((t_num, b_num))
            else:
                # every round, we need to add len(data) to edge_to(as well as edge_from) index
                increase_idx = len(feature_vectors)
                edge_to = np.concatenate((edge_to, edge_to_t + increase_idx), axis=0)
                edge_from = np.concatenate((edge_from, edge_from_t + increase_idx), axis=0)
                # normalize weight to be in range (0, 1)
                weight = np.concatenate((weight, weight_t), axis=0)
                probs_t = weight_t / weight_t.max()
                probs = np.concatenate((probs, probs_t), axis=0)
                sigmas = np.concatenate((sigmas, sigmas_t), axis=0)
                rhos = np.concatenate((rhos, rhos_t), axis=0)
                feature_vectors = np.concatenate((feature_vectors, fitting_data), axis=0)
                attention = np.concatenate((attention, attention_t), axis=0)
                knn_indices = np.concatenate((knn_indices, knn_idxs_t+increase_idx), axis=0)
                time_step_nums.append((t_num, b_num))

        # boundary points...

        # time_complex = construct_temporal_complex(X=feature_vectors,
        #                                         time_step_nums=time_step_nums,
        #                                         time_step_idxs_list = time_step_idxs_list,
        #                                         persistent=TEMPORAL_PERSISTENT,
        #                                         time_steps=time_steps,
        #                                         knn_indices=knn_indices,
        #                                         sigmas=sigmas,
        #                                         rhos=rhos)
        # # normalize for symmetry reason
        # _, heads, tails, vals, _ = get_graph_elements(time_complex, n_epochs=self.n_epochs)

        # weight = np.concatenate((weight, vals), axis=0)
        # probs_t = vals / (vals.max() + 1e-4)
        # probs = np.concatenate((probs, probs_t), axis=0)
        # edge_to = np.concatenate((edge_to, heads), axis=0)
        # edge_from = np.concatenate((edge_from, tails), axis=0)

        return edge_to, edge_from, probs, feature_vectors, time_step_nums, time_step_idxs_list, knn_indices , sigmas, rhos, attention
    

class kcSpatialEdgeConstructor(SpatialEdgeConstructor):
    def __init__(self, data_provider, init_num, n_epochs, n_neighbors, MAX_HAUSDORFF, ALPHA, BETA) -> None:
        super().__init__(data_provider, init_num, n_epochs, n_neighbors)
        self.MAX_HAUSDORFF = MAX_HAUSDORFF
        self.ALPHA = ALPHA
        self.BETA = BETA
    
    def _get_unit(self, data, adding_num=100):
        t0 = time.time()
        l = len(data)
        idxs = np.random.choice(np.arange(l), size=self.init_num, replace=False)
        # _,_ = hausdorff_dist_cus(data, idxs)

        id = IntrinsicDim(data)
        d0 = id.twonn_dimension_fast()
        # d0 = twonn_dimension_fast(data)

        kc = kCenterGreedy(data)
        _ = kc.select_batch_with_budgets(idxs, adding_num)
        c0 = kc.hausdorff()
        t1 = time.time()
        return c0, d0, "{:.1f}".format(t1-t0)
    
    def construct(self):
        """construct spatio-temporal complex and get edges

        Returns
        -------
        _type_
            _description_
        """

        # dummy input
        edge_to = None
        edge_from = None
        sigmas = None
        rhos = None
        weight = None
        probs = None
        feature_vectors = None
        attention = None
        knn_indices = None
        time_step_nums = list()
        time_step_idxs_list = list()

        train_num = self.data_provider.train_num
        selected_idxs = np.random.choice(np.arange(train_num), size=self.init_num, replace=False)

        baseline_data = self.data_provider.train_representation(self.data_provider.e)
        max_x = np.linalg.norm(baseline_data, axis=1).max()
        baseline_data = baseline_data/max_x
        
        c0,d0,_ = self._get_unit(baseline_data)

        # each time step
        for t in range(self.data_provider.e, self.data_provider.s - 1, -self.data_provider.p):
            print("=================+++={:d}=+++================".format(t))
            # load train data and border centers
            train_data = self.data_provider.train_representation(t).squeeze()

            # normalize data by max ||x||_2
            max_x = np.linalg.norm(train_data, axis=1).max()
            train_data = train_data/max_x

            # get normalization parameters for different epochs
            c,d,_ = self._get_unit(train_data)
            c_c0 = math.pow(c/c0, self.BETA)
            d_d0 = math.pow(d/d0, self.ALPHA)
            print("Finish calculating normaling factor")

            kc = kCenterGreedy(train_data)
            _ = kc.select_batch_with_cn(selected_idxs, self.MAX_HAUSDORFF, c_c0, d_d0, p=0.95)
            selected_idxs = kc.already_selected.astype("int")
            with open("selected_{}.json".format(t), "w") as f:
                json.dump(selected_idxs.tolist(), f)
            print("select {:d} points".format(len(selected_idxs)))

            time_step_idxs_list.insert(0, np.arange(len(selected_idxs)).tolist())

            train_data = self.data_provider.train_representation(t).squeeze()
            train_data = train_data[selected_idxs]
            border_centers = self.data_provider.border_representation(t).squeeze()

            t_num = len(selected_idxs)
            b_num = len(border_centers)

            complex, sigmas_t1, rhos_t1, knn_idxs_t = self._fuzzy_complex(train_data)
            bw_complex, sigmas_t2, rhos_t2, _ = self._boundary_wise_complex(train_data, border_centers)
            edge_to_t, edge_from_t, weight_t = self._construct_step_edge_dataset(complex, bw_complex)
            sigmas_t = np.concatenate((sigmas_t1, sigmas_t2[len(sigmas_t1):]), axis=0)
            rhos_t = np.concatenate((rhos_t1, rhos_t2[len(rhos_t1):]), axis=0)
            fitting_data = np.concatenate((train_data, border_centers), axis=0)
            pred_model = self.data_provider.prediction_function(t)
            attention_t = get_attention(pred_model, fitting_data, temperature=.01, device=self.data_provider.DEVICE, verbose=1)

            if edge_to is None:
                edge_to = edge_to_t
                edge_from = edge_from_t
                weight = weight_t
                probs = weight_t / weight_t.max()
                feature_vectors = fitting_data
                attention = attention_t
                sigmas = sigmas_t
                rhos = rhos_t
                knn_indices = knn_idxs_t
                # npr = npr_t
                time_step_nums.insert(0, (t_num, b_num))
            else:
                # every round, we need to add len(data) to edge_to(as well as edge_from) index
                increase_idx = len(fitting_data)
                edge_to = np.concatenate((edge_to_t, edge_to + increase_idx), axis=0)
                edge_from = np.concatenate((edge_from_t, edge_from + increase_idx), axis=0)
                # normalize weight to be in range (0, 1)
                weight = np.concatenate((weight_t, weight), axis=0)
                probs_t = weight_t / weight_t.max()
                probs = np.concatenate((probs_t, probs), axis=0)
                sigmas = np.concatenate((sigmas_t, sigmas), axis=0)
                rhos = np.concatenate((rhos_t, rhos), axis=0)
                feature_vectors = np.concatenate((fitting_data, feature_vectors), axis=0)
                attention = np.concatenate((attention_t, attention), axis=0)
                knn_indices = np.concatenate((knn_idxs_t, knn_indices+increase_idx), axis=0)
                # npr = np.concatenate((npr_t, npr), axis=0)
                time_step_nums.insert(0, (t_num, b_num))

        # boundary points...
        # time_complex = construct_temporal_complex(X=feature_vectors,
        #                                         time_step_nums=time_step_nums,
        #                                         time_step_idxs_list = time_step_idxs_list,
        #                                         persistent=TEMPORAL_PERSISTENT,
        #                                         time_steps=time_steps,
        #                                         knn_indices=knn_indices,
        #                                         sigmas=sigmas,
        #                                         rhos=rhos)
        # # normalize for symmetry reason
        # _, heads, tails, vals, _ = get_graph_elements(time_complex, n_epochs=NUMS)

        # # increase weight of temporal edges
        # # strenthen_neighbor = npr[heads]
        # weight = np.concatenate((weight, vals), axis=0)

        # probs_t = vals / (vals.max() + 1e-4)
        # # probs_t = probs_t*(1+strenthen_neighbor)
        # # probs_t = probs_t*TEMPORAL_EDGE_WEIGHT

        # probs = np.concatenate((probs, probs_t), axis=0)
        # edge_to = np.concatenate((edge_to, heads), axis=0)
        # edge_from = np.concatenate((edge_from, tails), axis=0)

        return edge_to, edge_from, probs, feature_vectors, time_step_nums, time_step_idxs_list, knn_indices, sigmas, rhos, attention